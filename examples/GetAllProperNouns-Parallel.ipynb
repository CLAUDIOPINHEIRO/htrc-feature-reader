{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to run the [proper noun counting example](/GetAllProperNouns.ipynb) in parallel using iPython Parallel.\n",
    "\n",
    "To start, you'll want to install iPython Parallel,\n",
    "\n",
    "`pip install ipyparallel`\n",
    "\n",
    "and start a controller in the same folder as this script. This example creates 4 nodes:\n",
    "\n",
    "`ipcluster start -n 4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paths = glob.glob(os.path.join('..','data','PZ-volumes', '*.basic.json.bz2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing the normal, single-thread code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "def get_proper_nouns_normal(vol):\n",
    "    tl = vol.tokenlist(pages=False)\n",
    "    tl.index = tl.index.droplevel(0)\n",
    "    tl['date'] = vol.year\n",
    "    tl = tl.set_index('date', append=True).reorder_levels(['date', 'token', 'pos'])\n",
    "    try:\n",
    "        proper_nouns = tl.loc[idx[:,:,('NNP', 'NNPS')],]\n",
    "        proper_nouns.index = proper_nouns.index.droplevel(2)\n",
    "        return proper_nouns[proper_nouns['count'] > 1]\n",
    "    except:\n",
    "        return pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 7.74 s per loop\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    fr = FeatureReader(paths)\n",
    "    nnp_dfs = []\n",
    "    for vol in fr.volumes():\n",
    "        nnp_dfs.append(get_proper_nouns_normal(vol))\n",
    "    all_nnp = pd.concat(nnp_dfs)\n",
    "%timeit test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing the ipyparallel code (4 engines, one volume at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipyparallel import Client, require\n",
    "# Create client\n",
    "c= Client()\n",
    "# Create load Balanced view\n",
    "lview = c.load_balanced_view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function, which takes a path, creates a Feature reader for just that path, gets the volume and processes it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@lview.parallel()\n",
    "@require('pandas', 'htrc_features')\n",
    "def get_proper_nouns(path):\n",
    "    idx = pandas.IndexSlice\n",
    "    vol = next(htrc_features.FeatureReader(path).volumes())\n",
    "    tl = vol.tokenlist(pages=False)\n",
    "    tl.index = tl.index.droplevel(0)\n",
    "    tl['date'] = vol.year\n",
    "    tl = tl.set_index('date', append=True).reorder_levels(['date', 'token', 'pos'])\n",
    "    try:\n",
    "        proper_nouns = tl.loc[idx[:,:,('NNP', 'NNPS')],]\n",
    "        proper_nouns.index = proper_nouns.index.droplevel(2)\n",
    "        return proper_nouns[proper_nouns['count'] > 1]\n",
    "    except:\n",
    "        return pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit pd.concat(get_proper_nouns.map(paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 5.6x times faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing the ipyparallel code (4 engines, 4 volumes at a time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requiring libraries is the main time bottleneck for this library. I'm not sure if ipyparallel does anything fancy to mitigate the `import` time, but ideally, we wouldn't send one path to an engine at a time. If we were processing lots of data, I would probably do ~100 volumes at a time. Since we're testing with just 15 volumes, let's see if there's a speed improvement by sending 4 paths at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paths_per_engine = 4\n",
    "multipaths = [paths[i::paths_per_engine] for i in range(1,paths_per_engine+1)]\n",
    "multipaths[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same function as before, but it now can return info from multiple Volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@lview.parallel()\n",
    "@require('pandas', 'htrc_features')\n",
    "def get_proper_nouns_multi(paths):\n",
    "    idx = pandas.IndexSlice\n",
    "    fr = htrc_features.FeatureReader(paths)\n",
    "    dfs = []\n",
    "    for vol in fr.volumes():\n",
    "        tl = vol.tokenlist(pages=False)\n",
    "        tl.index = tl.index.droplevel(0)\n",
    "        tl['date'] = vol.year\n",
    "        tl = tl.set_index('date', append=True).reorder_levels(['date', 'token', 'pos'])\n",
    "        try:\n",
    "            proper_nouns = tl.loc[idx[:,:,('NNP', 'NNPS')],]\n",
    "            proper_nouns.index = proper_nouns.index.droplevel(2)\n",
    "            dfs.append(proper_nouns[proper_nouns['count'] > 1])\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        return pd.concat(dfs)\n",
    "    except:\n",
    "        return pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit pd.concat(get_proper_nouns_multi.map(multipaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat trivial improvement, though this may potentially be greater on larger datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
