{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all Proper Nouns\n",
    "\n",
    "This example shows how one might gather all the proper nouns from a collection of books using the HTRC Feature Reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, collect the list of files that you hope to extract the nouns from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "paths = glob.glob('../data/PZ-volumes/*.basic.json.bz2')\n",
    "fr = FeatureReader(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's walk through what we would do with just one volume. We'll set the first volume of the FeatureReader to `vol` and return a tokenlist, without page level information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vol = next(fr.volumes())\n",
    "tl = vol.tokenlist(pages=False)\n",
    "tl[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm interested in the occurance of words across years, so we'll add a `date` column and absorb it into the MultiIndex as a new level. At the same time, we'll drop the `section` level, since it's all redundant information. You can read about [Pandas MultIndexes in the Pandas documentation](pandas.pydata.org/pandas-docs/stable/advanced.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove 'section'\n",
    "tl.index = tl.index.droplevel(0)\n",
    "# Add date column, convert to index level, and reorder levels\n",
    "tl['date'] = vol.year\n",
    "tl = tl.set_index('date', append=True).reorder_levels(['date', 'token', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the DataFrame looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tl[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get all the proper nouns, we'll 'slice' all the columns that have `NNP` or `NNPS` as the part-of-speech (POS) value.\n",
    "\n",
    "Slicing involves using the `.loc[]` to ask for, in order: all `date` rows, all `token` rows, and just `pos` rows that match `NNP` or `NNPS`. Below I use `IndexSlice` simply for a more familiar syntax, but `idx[:,:,('NNP', 'NNPS')]` is equivalent to asking for `(slice(None),slice(None),('NNP', 'NNPS'))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "proper_nouns = tl.loc[idx[:,:,('NNP', 'NNPS')],]\n",
    "# Show only proper nouns that occur more than once\n",
    "proper_nouns[proper_nouns['count'] > 1].sort_values('count', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Let's collect the info for all our volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "\n",
    "def get_proper_nouns(vol):\n",
    "    tl = vol.tokenlist(pages=False)\n",
    "    tl.index = tl.index.droplevel(0)\n",
    "    tl['date'] = vol.year\n",
    "    tl = tl.set_index('date', append=True).reorder_levels(['date', 'token', 'pos'])\n",
    "    try:\n",
    "        proper_nouns = tl.loc[idx[:,:,('NNP', 'NNPS')],]\n",
    "        proper_nouns.index = proper_nouns.index.droplevel(2)\n",
    "        return proper_nouns[proper_nouns['count'] > 1]\n",
    "    except:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect all results in a list, then concat the dataframes together\n",
    "nnp_dfs = []\n",
    "for vol in fr.volumes():\n",
    "    nnp_dfs.append(get_proper_nouns(vol))\n",
    "all_nnp = pd.concat(nnp_dfs)\n",
    "del nnp_dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_nnp = pd.concat(nnp_dfs)\n",
    "all_nnp.sort_values('count', ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, these counts are biased by the fact that there are only 15 books in the sample. Let's look at what terms occurred at least twice is the most number of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_nnp['occurred'] = 1\n",
    "all_nnp.reset_index().groupby(['token']).sum()\\\n",
    "       .sort_values('occurred', ascending=False)[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
