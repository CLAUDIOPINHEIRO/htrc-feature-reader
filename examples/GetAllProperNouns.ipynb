{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all Proper Nouns\n",
    "\n",
    "This example shows how one might gather all the proper nouns from a collection of books using the HTRC Feature Reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, collect the list of files that you hope to extract the nouns from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "paths = glob.glob('../data/PZ-volumes/*.basic.json.bz2')\n",
    "fr = FeatureReader(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's walk through what we would do with just one volume. We'll set the first volume of the FeatureReader to `vol` and return a tokenlist, without page level information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <th>!</th>\n",
       "      <th>.</th>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count\n",
       "section token pos       \n",
       "body    !     .      279"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = next(fr.volumes())\n",
    "tl = vol.tokenlist(pages=False)\n",
    "tl[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm interested in the occurance of words across years, so we'll add a `date` column and absorb it into the MultiIndex as a new level. At the same time, we'll drop the `section` level, since it's all redundant information. You can read about [Pandas MultIndexes in the Pandas documentation](pandas.pydata.org/pandas-docs/stable/advanced.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove 'section'\n",
    "tl.index = tl.index.droplevel(0)\n",
    "# Add date column, convert to index level, and reorder levels\n",
    "tl['date'] = vol.year\n",
    "tl = tl.set_index('date', append=True).reorder_levels(['date', 'token', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the DataFrame looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <th>!</th>\n",
       "      <th>.</th>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                count\n",
       "date token pos       \n",
       "1901 !     .      279"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get all the proper nouns, we'll 'slice' all the columns that have `NNP` or `NNPS` as the part-of-speech (POS) value.\n",
    "\n",
    "Slicing involves using the `.loc[]` to ask for, in order: all `date` rows, all `token` rows, and just `pos` rows that match `NNP` or `NNPS`. Below I use `IndexSlice` simply for a more familiar syntax, but `idx[:,:,('NNP', 'NNPS')]` is equivalent to asking for `(slice(None),slice(None),('NNP', 'NNPS'))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">1901</th>\n",
       "      <th>Carmela</th>\n",
       "      <th>NNP</th>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gigli</th>\n",
       "      <th>NNP</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roberto</th>\n",
       "      <th>NNP</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Traetta</th>\n",
       "      <th>NNP</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rocco</th>\n",
       "      <th>NNP</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naples</th>\n",
       "      <th>NNP</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gargiulo</th>\n",
       "      <th>NNP</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Captain</th>\n",
       "      <th>NNP</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cecilia</th>\n",
       "      <th>NNP</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minino</th>\n",
       "      <th>NNP</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count\n",
       "date token    pos       \n",
       "1901 Carmela  NNP    236\n",
       "     Gigli    NNP     77\n",
       "     Roberto  NNP     59\n",
       "     Traetta  NNP     59\n",
       "     Rocco    NNP     58\n",
       "     Naples   NNP     58\n",
       "     Gargiulo NNP     53\n",
       "     Captain  NNP     53\n",
       "     Cecilia  NNP     42\n",
       "     Minino   NNP     41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = pd.IndexSlice\n",
    "proper_nouns = tl.loc[idx[:,:,('NNP', 'NNPS')],]\n",
    "# Show only proper nouns that occur more than once\n",
    "proper_nouns[proper_nouns['count'] > 1].sort_values('count', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Let's collect the info for all our volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "\n",
    "def get_proper_nouns(vol):\n",
    "    tl = vol.tokenlist(pages=False)\n",
    "    tl.index = tl.index.droplevel(0)\n",
    "    tl['date'] = vol.year\n",
    "    tl = tl.set_index('date', append=True).reorder_levels(['date', 'token', 'pos'])\n",
    "    try:\n",
    "        proper_nouns = tl.loc[idx[:,:,('NNP', 'NNPS')],]\n",
    "        proper_nouns.index = proper_nouns.index.droplevel(2)\n",
    "        return proper_nouns[proper_nouns['count'] > 1]\n",
    "    except:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect all results in a list, then concat the dataframes together\n",
    "nnp_dfs = []\n",
    "for vol in fr.volumes():\n",
    "    nnp_dfs.append(get_proper_nouns(vol))\n",
    "all_nnp = pd.concat(nnp_dfs)\n",
    "del nnp_dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nnp_dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bb3ac6284c06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_nnp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnp_dfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mall_nnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nnp_dfs' is not defined"
     ]
    }
   ],
   "source": [
    "all_nnp = pd.concat(nnp_dfs)\n",
    "all_nnp.sort_values('count', ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, these counts are biased by the fact that there are only 15 books in the sample. Let's look at what terms occurred at least twice is the most number of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_nnp['occurred'] = 1\n",
    "all_nnp.reset_index().groupby(['token']).sum()\\\n",
    "       .sort_values('occurred', ascending=False)[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
