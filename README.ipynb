{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTRC-Features\n",
    "=============\n",
    "\n",
    "Tools for working with HTRC Feature Extraction files\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install,\n",
    "\n",
    "    git clone https://github.com/organisciak/htrc-feature-reader.git\n",
    "    cd htrc-feature-reader\n",
    "    python setup.py install\n",
    "\n",
    "That's it! This library is written for Python 2.7 and 3.0+.\n",
    "\n",
    "Two optional modules improve the HTRC-Feature-Reader: `pysolr` allows fetching of metadata, and `ujson` speeds up loading by about 0.4s per file. To install:\n",
    "\n",
    "    pip install pysolr ujson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "### Reading feature files\n",
    "\n",
    "The easiest way to start using this library is to use the `FeatureReader`, which takes a list of paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from htrc_features import FeatureReader\n",
    "paths = glob.glob('data/PZ-volumes/*basic.json.bz2')\n",
    "# Here we're loading five paths, for brevity\n",
    "feature_reader = FeatureReader(paths[:5])\n",
    "i = 0\n",
    "for vol in feature_reader.volumes():\n",
    "    print(\"%s - %s\" % (vol.id, vol.title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating on `FeatureReader.volumes()` returns `Volume` objects.\n",
    "Wherever possible, this library tries not to hold things in memory, so most of the time you want to iterate rather than casting to a list.\n",
    "In addition to memory issues, since each volume needs to be read from a file and initialized, it will be slow. \n",
    "_Woe to whomever tries `list(FeatureReader.volumes())`_.\n",
    "\n",
    "The method for creating a path list with 'glob' is just one way to do so.\n",
    "For large sets, it's better to just have a text file of your paths, and read it line by line.\n",
    "\n",
    "The feature reader also has a useful method, `multiprocessing(map_func)`, for chunking a running functions across multiple processes.\n",
    "This is an advanced feature, but extremely helpful for any large-scale processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume\n",
    "\n",
    "A volume contains information about the current work and access to the pages of the work.\n",
    "\n",
    "All the metadata fields from the HTRC JSON file are accessible as properties of the volume object, including _title_, _language_, _imprint_, _oclc_, _pubDate_, and _genre_. The main identifier _id_ and _pageCount_ are also accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"Volume %s has %s pages in %s\" % (vol.id, vol.pageCount, vol.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a convenience, Volume.year returns Volume.pubDate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"%s == %s\" % (vol.pubDate, vol.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the feature_reader, it doubles as a generator for pages, and again, it's preferable for speed and memory to iterate over the pages than to read them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's skip ahead some pages\n",
    "i = 0\n",
    "for page in vol:\n",
    "    i += 1\n",
    "    if i >= 16:\n",
    "        break\n",
    "        \n",
    "print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a pleasant way to access `vol.pages()`.\n",
    "If you want to pass arguments to page initialization, such as changing the pages default section from body to 'fullpage', it can be done with `for page in vol.pages(default_section='fullpage')`. \n",
    "\n",
    "Finally, if the minimal metadata included with the extracted feature files is insufficient, you can fetch the HTRC's metadata record with `vol.metadata`.\n",
    "Remember that this calls the HTRC servers for each volume, so can add considerable overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr = FeatureReader(paths[0:5])\n",
    "for vol in fr.volumes():\n",
    "    print(vol.metadata['published'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"METADATA FIELDS: \" + \", \".join(vol.metadata.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_At large-scales, using `vol.metadata` is an impolite and inefficient amount of server pinging; there are better ways to query the API than one-by-one._\n",
    "\n",
    "## Pages and Sections\n",
    "\n",
    "A page contains the meat of the HTRC's extracted features.\n",
    "Since the HTRC provides information by header/body/footer, these are accessed as separate 'sections' with `Page.header`, `Page.body`, and `Page.footer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The body has %s lines and %s sentences\" % (page.body.lineCount, page.body.sentenceCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also `Page.fullpage`, which is a section combining the header, footer, and body.\n",
    "Remember that these need to be added together, which isn't done until the first time `fullpage` is accessed, and in large-scale processing those milliseconds can add up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullpage = page.fullpage\n",
    "combined_token_count = page.body.tokenCount + page.header.tokenCount + page.footer.tokenCount\n",
    "# check that full page is adding properly\n",
    "assert(fullpage.tokenCount == combined_token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, the properties of the page and section are identical to the HTRC Extracted Features schema, rather than following Python naming conventions (e.g. CamelCase when convention would expect underscore_separation).\n",
    "\n",
    "A page has a default section, where some features -- such as accessing a token list -- can be accessed without specify the section each time. For example, with the default_section set to 'body', as it is by default, `Page.body.tokenlist` can be accessed with `Page.tokenlist`.\n",
    "\n",
    "## The fun stuff: playing with token counts and character counts\n",
    "\n",
    "Token lists are contained in Section.tokenlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tl = page.body.tokenlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `tokenlist` returns a [Pandas](http://pandas.pydata.org/) DataFrame through `tokenlist.token_counts()`, and provides syntactic access to the vocabulary (`tokenlist.tokens`) and a total token count (`tokenlist.count`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = tl.token_counts()\n",
    "print(df.sort_values(by='count', ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be manipulated in various ways. You can case-fold, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = tl.token_counts(case=False)\n",
    "print(df.sort_values(by='count', ascending=False)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you can combine part of speech counts into a single integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = tl.token_counts(pos=False)\n",
    "print(df.sort_values(by='count', ascending=False)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get just the unique tokens, `TokenList.tokens` provides them, though it is just an easy way to get `TokenList.token_counts().keys()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tl.tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to token lists, you can also access `Section.beginLineChars` and `Section.endLineChars`, which are dictionaries of character counts that occur at the start or end of the line.\n",
    "\n",
    "### Volume stats collecting\n",
    "\n",
    "The Volume object has a number of methods for collecting information from all its pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = vol.tokens_per_page()\n",
    "# Show first 15 pages\n",
    "tokens[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = vol.term_page_freqs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a.iloc[1:3, 4:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vol.term_volume_freqs()[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume.term_page_freqs provides a wide DataFrame resembling a matrix, where terms are listed as columns, pages are listed as rows, and the values correspond to the term frequency (or page page frequency with `page_freq=true`).\n",
    "Volume.term_volume_freqs() simply sums these.\n",
    " \n",
    "### Multiprocessing\n",
    "\n",
    "For faster processing, you can write a mapping function for acting on volumes, then pass it to `FeatureReader.multiprocessing`.\n",
    "This sends out the function to a different process per volume, spawning (CPU_CORES-1) processes at a time.\n",
    "The map function receives the feature_reader and a volume path, and needs to initialize the volume.\n",
    "\n",
    "Here's a simple example that returns the term counts for each volume (take note of the first two lines of the functions):\n",
    "\n",
    "```python\n",
    "def printTermCounts(args):\n",
    "    fr, path = args\n",
    "    vol = fr.create_volume(path)\n",
    "\n",
    "    metadata = (vol.id, vol.year)\n",
    "    return (metadata, results)\n",
    "\n",
    "    results = feature_reader.multiprocessing(map_func)\n",
    "    for vol, result in results:\n",
    "\t\tprint(\"Results from %s (%d)\" % vol)\n",
    "\t\tfor term, count in result.items():\n",
    "            print(\"%s: %d\" % (term, count))\n",
    "```\n",
    "\n",
    "Some rules: results must be serializeable, and the map_func must be accessible from __main__ (basically: no dynamic functions: they should be written plainly in your script).\n",
    "\n",
    "The results are collected and returned together, so you don't want a feature reader with all 250k files, because the results will be too much memory (depending on how big your result is).\n",
    "Instead, it easier to initialize feature readers for smaller batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Files\n",
    "\n",
    "In the beta Extracted Features release, schema 2.0, a few features were separated out to an advanced files. If you try to access those features, like `endLineChars`, you'll get a error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end_line_chars = vol.end_line_chars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to load the advanced file alongside the basic files by passing in a `(basic, advanced)` tuple of filepaths where you would normally pass in a single path. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newpaths = [(x,x.replace('basic', 'advanced')) for x in paths]\n",
    "newpaths[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr = FeatureReader(newpaths)\n",
    "vol = next(fr.volumes())\n",
    "end_line_chars = vol.end_line_chars()\n",
    "print(end_line_chars['!'][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the advanced files are not fully supported, because the basic/advanced split will not continue for future releases.\n",
    "\n",
    "Loading and parsing the advanced feature files adds non negligible time (about `1.3` seconds on my computer), so only load them if you need them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
